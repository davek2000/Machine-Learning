import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
from sklearn.kernel_ridge import KernelRidge
from sklearn.metrics import r2_score
from sklearn.dummy import DummyRegressor

#Import Data
df=pd.read_csv("Defenders.csv")
# print(df.head())
X1=df.iloc[:,2]
X2=df.iloc[:,3]
X3=df.iloc[:,4]
X4=df.iloc[:,5]
X5=df.iloc[:,6]
X6=df.iloc[:,7]
X7=df.iloc[:,8]
X8=df.iloc[:,9]
X9=df.iloc[:,10]
X10=df.iloc[:,11]
X11=df.iloc[:,13]
X11=X11/10000000
X=np.column_stack((X1,X2,X3,X4,X5,X6,X7,X8,X9,X10,X11))
y=df.iloc[:,12]
y=y/10000000

#Split test and train data with ratio 2/8
Xtrain, Xtest, ytrain, ytest = train_test_split(X,y,test_size=0.2)
C=10;
model = KernelRidge(alpha=1.0/C, kernel='linear', gamma=10).fit(Xtrain, ytrain)
ypred = model.predict(Xtest)
error_final=mean_squared_error(ytest,ypred)
error_final2=r2_score(ytest,ypred)
print(error_final)
print(error_final2)

#basline predictor
dummy = DummyRegressor(strategy="mean").fit(Xtrain, ytrain)
ydummy = dummy.predict(Xtest)
error_base=mean_squared_error(ytest,ydummy)
error_base2=r2_score(ytest,ydummy)
print(error_base)
print(error_base2)
